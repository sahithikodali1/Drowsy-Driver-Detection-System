{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTbase_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahithikodali1/Drowsy-Driver-Detection-System/blob/master/BERTbase_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jymts-aLDSIJ"
      },
      "source": [
        "#Mounting the drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_bsiLbS4jT4"
      },
      "source": [
        "#import the libraries\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "# BERT imports\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import copy\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "!pip install transformers\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# specify GPU device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "#Data file path\n",
        "DATA_DIR = \"/content/drive/MyDrive/Thesis_B\"\n",
        "file = '8b_data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0owgoQMvEik"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Tokenize with BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJj7zIEd2Qth"
      },
      "source": [
        "#Function to find the datasize\n",
        "def find_datasize(path,filename):\n",
        "    file = pd.read_csv(os.path.join(path,filename))\n",
        "    f_values = file.values\n",
        "    q_id_split = np.split(f_values, np.where(np.diff(f_values[:,0]))[0]+1)\n",
        "    col_names = file.columns\n",
        "    print(col_names)\n",
        "    print(file[:10])\n",
        "    train_data_size = round(len(q_id_split)*0.8)\n",
        "    val_data_size = round(len(q_id_split) - train_data_size)\n",
        "    print(\"Training data size:{}\".format(train_data_size))\n",
        "    print(\"Validation data size:{}\".format(val_data_size))\n",
        "    return q_id_split,train_data_size,val_data_size,col_names\n",
        "\n",
        "#Function to split data into train and validation data based on questionID's randomly\n",
        "def split_dataframes(q_id_split,train_data_size,col_names):\n",
        "    random.seed(3007)\n",
        "    random.shuffle(q_id_split)\n",
        "    train_data = q_id_split[:train_data_size]\n",
        "    val_data = q_id_split[train_data_size:]\n",
        "    f_values_train = np.concatenate(train_data, axis=0)\n",
        "    f_values_val = np.concatenate(val_data, axis=0)\n",
        "    print(len(train_data))\n",
        "    print(len(val_data))\n",
        "    train_df = pd.DataFrame(f_values_train, columns = col_names)\n",
        "    val_df = pd.DataFrame(f_values_val, columns = col_names)\n",
        "    return train_df,val_df\n",
        "\n",
        "#Function to obtain labels list\n",
        "def obtain_SU4labels_list(dataframe):\n",
        "    SU4_labels = dataframe['SU4_labels']\n",
        "    labels_list = list(SU4_labels)\n",
        "    print('Labels size:{}'.format(len(labels_list)))\n",
        "    return labels_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPtuKVvyiCLB"
      },
      "source": [
        "#Splitting the data and obtaining labels\n",
        "q_id_split, train_data_size, val_data_size, col_names = find_datasize(DATA_DIR,file)\n",
        "train_df, val_df = split_dataframes(q_id_split,train_data_size,col_names)\n",
        "train_labels = obtain_SU4labels_list(train_df)\n",
        "val_labels = obtain_SU4labels_list(val_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roHVSbVZoh_7"
      },
      "source": [
        "#Add special tokens\n",
        "def obtain_specialtokenized_list(dataframe):\n",
        "    sentences = dataframe['sentence text']\n",
        "    questions = dataframe['question']\n",
        "    sentences_list = list(sentences)\n",
        "    questions_list = list(questions)\n",
        "    question_sentence_list = []\n",
        "    for i in range(len(sentences_list)):\n",
        "        question_sentence_list = question_sentence_list + [\"[CLS] \" + questions_list[i] + \" [SEP] \" + sentences_list[i] + \" [SEP]\"]\n",
        "    return question_sentence_list\n",
        "\n",
        "#Tokenize texts\n",
        "def tokenize_sent(givenlist):\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in givenlist]\n",
        "    return tokenized_texts\n",
        "\n",
        "#Replace commas from tokens\n",
        "def remove_token_commas(givenlist):\n",
        "  no_commas_tokenlist = []\n",
        "  for each in givenlist:\n",
        "    y = [i.replace(',',';') for i in each]\n",
        "    no_commas_tokenlist.append(y)\n",
        "  return no_commas_tokenlist\n",
        "\n",
        "#Create segment ids from tokens\n",
        "def segment_id(givenlist):\n",
        "  MAX_LEN = 512\n",
        "  segment_ids = []\n",
        "  for each in givenlist:\n",
        "    token_sent = ','.join(each)\n",
        "    d = \"[SEP]\"\n",
        "    ques_ans =  [token+d for token in token_sent.split(d) if token]\n",
        "    ques = [0]*len(ques_ans[0].split(','))\n",
        "    ans = [1]*(len(ques_ans[1].split(','))-1)\n",
        "    seg_ids_ques_ans = ques+ans\n",
        "    length = len(seg_ids_ques_ans)\n",
        "    if length >= MAX_LEN:\n",
        "      length_current = MAX_LEN\n",
        "      seg_ids_ques_ans = seg_ids_ques_ans[:length_current]\n",
        "      segment_ids.append(seg_ids_ques_ans)\n",
        "    else:\n",
        "      length_current = MAX_LEN - length\n",
        "      seg_ids_ques_ans += [0]*length_current\n",
        "      segment_ids.append(seg_ids_ques_ans)\n",
        "  return segment_ids\n",
        "\n",
        "#Convert tokenized sentences to respective token ids\n",
        "def token2ids(tokenized_texts):\n",
        "    MAX_LEN = 512\n",
        "    tokens_to_ids = [tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_texts]\n",
        "    tokens_to_ids = pad_sequences(tokens_to_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    return tokens_to_ids\n",
        "\n",
        "#Create masks for the tokenids\n",
        "def create_masks(token_ids):\n",
        "    attention_masks = []\n",
        "    for tid in token_ids:\n",
        "        tid_mask = [float(i>0) for i in tid]\n",
        "        attention_masks.append(tid_mask)\n",
        "    return attention_masks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAPahYO2osPD"
      },
      "source": [
        "#Obtain the tokens, converting to inputs required\n",
        "train_specialtok_list = obtain_specialtokenized_list(train_df)  \n",
        "val_specialtok_list = obtain_specialtokenized_list(val_df)  \n",
        "\n",
        "train_tokenized = tokenize_sent(train_specialtok_list)\n",
        "val_tokenized = tokenize_sent(val_specialtok_list)\n",
        "\n",
        "train_tokenized_nocommas = remove_token_commas(train_tokenized)\n",
        "val_tokenized_nocommas = remove_token_commas(val_tokenized)\n",
        "\n",
        "train_token_type_ids =  segment_id(train_tokenized_nocommas)\n",
        "val_token_type_ids =  segment_id(val_tokenized_nocommas)\n",
        "\n",
        "train_tokenids = token2ids(train_tokenized_nocommas)\n",
        "val_tokenids = token2ids(val_tokenized_nocommas)\n",
        "\n",
        "train_masks = create_masks(train_tokenids)\n",
        "val_masks = create_masks(val_tokenids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU8l-4B7pQ1K"
      },
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "train_inputs = torch.tensor(train_tokenids, dtype=torch.int64)\n",
        "validation_inputs = torch.tensor(val_tokenids, dtype=torch.int64)\n",
        "\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.int64)\n",
        "validation_labels = torch.tensor(val_labels, dtype=torch.int64)\n",
        "\n",
        "train_masks = torch.tensor(train_masks, dtype=torch.float32)\n",
        "validation_masks = torch.tensor(val_masks, dtype=torch.float32)\n",
        "\n",
        "train_token_type_ids = torch.tensor(train_token_type_ids, dtype=torch.int64)\n",
        "validation_token_type_ids = torch.tensor(val_token_type_ids, dtype=torch.int64)\n",
        "\n",
        "print(type(train_inputs))\n",
        "print(type(train_masks))\n",
        "print(type(train_labels))\n",
        "print(type(train_token_type_ids))\n",
        "print('***************')\n",
        "print(train_inputs.dtype)\n",
        "print(train_masks.dtype)\n",
        "print(train_labels.dtype)\n",
        "print(train_token_type_ids.dtype)\n",
        "print('***************')\n",
        "print(validation_inputs.dtype)\n",
        "print(validation_masks.dtype)\n",
        "print(validation_labels.dtype)\n",
        "print(validation_token_type_ids.shape)\n",
        "print('***************')\n",
        "print(train_inputs.shape)\n",
        "print(train_masks.shape)\n",
        "print(train_labels.shape)\n",
        "print(train_token_type_ids.shape)\n",
        "print('***************')\n",
        "print(train_inputs[0].shape)\n",
        "print(train_masks[0].shape)\n",
        "print(train_labels[0].shape)\n",
        "print(train_token_type_ids[0].shape)\n",
        "print('***************')\n",
        "print(train_inputs[0])\n",
        "print(train_masks[0])\n",
        "print(train_labels[0])\n",
        "print(train_token_type_ids[0])\n",
        "\n",
        "\n",
        "# Select a batch size for training. \n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader \n",
        "train_data = TensorDataset(train_inputs, train_token_type_ids, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_token_type_ids, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr7OCmvcqlZj"
      },
      "source": [
        "#Declare random seed value\n",
        "\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "#Functions to calculate evaluation metrics\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def flat_f1score(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return f1_score(labels_flat, pred_flat, average = 'weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuD8uF9xthGq"
      },
      "source": [
        "#Function to define model\n",
        "def model(type,pretrainedmodel):\n",
        "    model = type.from_pretrained(pretrainedmodel, num_labels = 2)\n",
        "    for param in model.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkl5a5NF4lyI"
      },
      "source": [
        "#Calling the desired model architecture\n",
        "from transformers import BertForSequenceClassification\n",
        "model = model(BertForSequenceClassification,'bert-base-uncased')\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u387HeE_tZ_s"
      },
      "source": [
        "#Define epochs\n",
        "epochs = 4\n",
        "\n",
        "#Optimizer & Scheduler fine-tuning parameters\n",
        "lr=2e-5\n",
        "num_warmup_steps = 10\n",
        "num_training_steps = 1000\n",
        "\n",
        "#Optmizer and Scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDQQmlK1QtNI"
      },
      "source": [
        "#Importing loss functions\n",
        "from torch.nn import CrossEntropyLoss, BCELoss, Sigmoid, BCEWithLogitsLoss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ln3ADK5qXMT"
      },
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "best_accuracy_val = 0\n",
        "best_epoch = -1\n",
        "best_epoch_weights = copy.deepcopy(model.state_dict())\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):  \n",
        "  ## TRAINING\n",
        "  # Set our model to training mode\n",
        "  model.train()  \n",
        "  tr_loss = 0\n",
        "  tr_accuracy, tr_f1score = 0, 0\n",
        "  nb_tr_steps = 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_tokenids, b_input_masks, b_labels = batch\n",
        "\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids = b_input_tokenids, attention_mask = b_input_masks, labels = None)\n",
        "\n",
        "    loss_fn = CrossEntropyLoss()\n",
        "    loss = loss_fn(outputs.logits, b_labels)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = outputs.logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    tr_loss += loss.item()\n",
        "    tmp_tr_accuracy = flat_accuracy(logits, label_ids)\n",
        "    tmp_tr_f1score = flat_f1score(logits, label_ids)    \n",
        "    tr_accuracy += tmp_tr_accuracy\n",
        "    tr_f1score += tmp_tr_f1score\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  epoch_loss = tr_loss/nb_tr_steps\n",
        "  epoch_accuracy = tr_accuracy/nb_tr_steps\n",
        "  epoch_f1score = tr_f1score/nb_tr_steps\n",
        "  train_loss_set.append(epoch_loss)\n",
        "\n",
        "  print(\"Train loss: {}\".format(epoch_loss))\n",
        "  print(\"Training Accuracy for epoch: {}\".format(epoch_accuracy))\n",
        "  print(\"Training f1score: {}\".format(epoch_f1score))\n",
        "\n",
        "  ##VALIDATION\n",
        "  model.eval()\n",
        "\n",
        "  val_loss_set = []\n",
        "  eval_loss, eval_accuracy, eval_f1score = 0, 0, 0\n",
        "  nb_eval_steps = 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "      batch = tuple(t.to(device) for t in batch)\n",
        "      \n",
        "      # Unpack the inputs from our dataloader\n",
        "      b_input_ids, b_input_tokenids, b_input_masks, b_labels = batch\n",
        "\n",
        "      # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "      with torch.no_grad():\n",
        "          outputs = model(b_input_ids, token_type_ids = b_input_tokenids, attention_mask = b_input_masks, labels = None)\n",
        "            \n",
        "      loss_fn = CrossEntropyLoss()\n",
        "      loss = loss_fn(outputs.logits, b_labels)\n",
        "\n",
        "      # Move logits and labels to CPU\n",
        "      logits = outputs.logits.detach().cpu().numpy()\n",
        "      label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "      eval_loss += loss.item()\n",
        "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "      tmp_eval_f1score = flat_f1score(logits, label_ids)\n",
        "      eval_accuracy += tmp_eval_accuracy\n",
        "      eval_f1score += tmp_eval_f1score\n",
        "      nb_eval_steps += 1\n",
        "\n",
        "      val_epoch_loss = eval_loss/nb_eval_steps\n",
        "      val_epoch_accuracy = eval_accuracy/nb_eval_steps\n",
        "      epoch_f1score = eval_f1score/nb_eval_steps\n",
        "      val_loss_set.append(val_epoch_loss)\n",
        "\n",
        "  print(\"Validation loss: {}\".format(val_epoch_loss))\n",
        "  print(\"Validation Accuracy: {}\".format(val_epoch_accuracy))\n",
        "  print(\"Validation f1score: {}\".format(epoch_f1score))\n",
        "\n",
        "  if (epoch_accuracy > best_accuracy_val):\n",
        "    best_accuracy_val = epoch_accuracy\n",
        "    best_epoch = epoch \n",
        "    torch.save(model.state_dict(), os.path.join(DATA_DIR, 'Bert-8b_5b_epoch_bccloss{}.pth'.format(epoch)))\n",
        "\n",
        "print(\" Best epoch: {}\".format(best_epoch))\n",
        "print(\" Best Accuracy: {}\".format(best_accuracy_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kj_CVHDw2NTb"
      },
      "source": [
        "#Model to load the saved best model\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Thesis_B/Bert-base-8b_5b_epoch_2_final.pth'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS4RdNy_0ufk"
      },
      "source": [
        "###TESTING DATA\n",
        "##Code to load jsonfile\n",
        "from pandas.io.json import json_normalize \n",
        "\n",
        "#Test file path\n",
        "test_file = '/content/drive/MyDrive/Thesis_B/8B1_golden.json'\n",
        "\n",
        "with open(test_file, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "##Code to get questions list\n",
        "def get_testdatalists(data):\n",
        "    data_access = data['questions']\n",
        "    id_list = []\n",
        "    type_list = []\n",
        "    body_list = []\n",
        "    all_text_list = []\n",
        "    for i in range(len(data_access)):\n",
        "      text_list = []\n",
        "      id_list.append(data_access[i]['id'])\n",
        "      type_list.append(data_access[i]['type'])\n",
        "      body_list.append(data_access[i]['body'])\n",
        "      for j in data_access[i]['snippets']:\n",
        "          text_list.append(j['text'])\n",
        "      all_text_list.append(text_list)\n",
        "    return id_list, body_list, type_list, all_text_list\n",
        "\n",
        "#Make a dataframe for test data\n",
        "def get_dataframes(id_list, body_list, type_list, all_text_list):\n",
        "    test_df = pd.DataFrame()\n",
        "    test_df['id'] = id_list\n",
        "    test_df['body'] = body_list\n",
        "    test_df['type'] = type_list\n",
        "    test_df['sentences'] = all_text_list\n",
        "    print(test_df[:10])\n",
        "    return test_df\n",
        "\n",
        "#Get list of questions and sentences\n",
        "def get_datalists(test_df):\n",
        "    qid_test_df = test_df['id']\n",
        "    type_test_df= test_df['type']\n",
        "    sentences_test_df = test_df['sentences']\n",
        "    questions_test_df = test_df['body']\n",
        "    qid_list_test_df = list(qid_test_df)\n",
        "    type_list_test_df = list(type_test_df)\n",
        "    sentences_list_test_df = list(sentences_test_df)\n",
        "    questions_list_test_df = list(questions_test_df)\n",
        "    return questions_list_test_df, sentences_list_test_df\n",
        "\n",
        "#Add special tokens\n",
        "def join_ques_sent(questions_list_test_df, sentences_list_test_df):\n",
        "    question_sentence_list_test_df = []\n",
        "    for i in range(len(questions_list_test_df)):\n",
        "      each_list = []\n",
        "      for j in sentences_list_test_df[i]:\n",
        "          each_list= each_list + [\"[CLS] \" + questions_list_test_df[i] + \" [SEP] \" + j + \" [SEP]\"]\n",
        "      question_sentence_list_test_df.append(each_list)\n",
        "    return question_sentence_list_test_df\n",
        "\n",
        "#Tokenize and create tokenids, typeids and masks\n",
        "def create_test_tokens_masks(question_sentence_list_test_df):\n",
        "    token_ids_test_df_all = []\n",
        "    type_ids_test_df_all = []\n",
        "    masks_test_df_all = []\n",
        "    for ques_ans_sent in question_sentence_list_test_df:\n",
        "        tokenized_test_df = tokenize_sent(ques_ans_sent)\n",
        "        no_commas_tokenlist_test_df = remove_token_commas(tokenized_test_df)\n",
        "        \n",
        "        token_ids_test_df = token2ids(no_commas_tokenlist_test_df)\n",
        "        masks_test_df = create_masks(token_ids_test_df)\n",
        "        type_ids_test_df = segment_id(no_commas_tokenlist_test_df)\n",
        "\n",
        "        token_ids_test_df_all.append(token_ids_test_df)\n",
        "        type_ids_test_df_all.append(type_ids_test_df)\n",
        "        masks_test_df_all.append(masks_test_df)\n",
        "    return token_ids_test_df_all, type_ids_test_df_all, masks_test_df_all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLxUoHZQ3332"
      },
      "source": [
        "#obtain inputs for model\n",
        "id_list, body_list, type_list, all_text_list = get_testdatalists(data)\n",
        "test_df = get_dataframes(id_list, body_list, type_list, all_text_list)\n",
        "\n",
        "questions_list_test_df, sentences_list_test_df = get_datalists(test_df)\n",
        "question_sentence_list_test_df = join_ques_sent(questions_list_test_df, sentences_list_test_df)\n",
        "\n",
        "token_ids_test_df_all, type_ids_test_df_all, masks_test_df_all = create_test_tokens_masks(question_sentence_list_test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gWH8YU3Aquv"
      },
      "source": [
        "print(len(token_ids_test_df_all))\n",
        "print(len(type_ids_test_df_all))\n",
        "print(len(masks_test_df_all))\n",
        "\n",
        "print(type(token_ids_test_df_all[0]))\n",
        "print(type(type_ids_test_df_all[0]))\n",
        "print(type(masks_test_df_all[0]))\n",
        "\n",
        "print(token_ids_test_df_all[0])\n",
        "print(type_ids_test_df_all[0])\n",
        "print(masks_test_df_all[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GzdNime34Kd"
      },
      "source": [
        "#Evaluating the model to produce a list of top sentences i.e summary\n",
        "topsentences_summary = []\n",
        "\n",
        "for i in range(len(token_ids_test_df_all)):\n",
        "  batch_size = len(token_ids_test_df_all[i])\n",
        "\n",
        "  test_inputs = torch.tensor(token_ids_test_df_all[i], dtype=torch.int64)\n",
        "  test_masks = torch.tensor(masks_test_df_all[i], dtype=torch.float32)\n",
        "  test_token_ids = torch.tensor(type_ids_test_df_all[i], dtype=torch.int64)\n",
        "\n",
        "  test_data = TensorDataset(test_inputs, test_token_ids, test_masks)\n",
        "  test_sampler = SequentialSampler(test_data)\n",
        "  test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
        "\n",
        "  logits_list_test_df = []\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in test_dataloader:\n",
        "\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_tokenids, b_input_masks = batch\n",
        "    \n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      \n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids = b_input_tokenids, attention_mask = b_input_masks, labels = None)   \n",
        "      logits_list_test_df.append(outputs.logits)\n",
        "    \n",
        "    prediction = torch.sigmoid(outputs.logits)\n",
        "    print(\"Softmax:{}\".format(prediction))\n",
        "\n",
        "    #Code to extract top 5 sentences based on probabilities\n",
        "    if len(prediction) >= 5:\n",
        "      values, indices = torch.topk(prediction,5,dim=0)\n",
        "      print(\"top_indices:{}\".format(indices))\n",
        "      indexes = indices[:,0].tolist()\n",
        "      each_summ = []\n",
        "      for ind in indexes:\n",
        "        each_summ.append(test_df['sentences'][i][ind])\n",
        "      each_summ = ' '.join(map(str, each_summ))\n",
        "      topsentences_summary.append(each_summ)\n",
        "      print(each_summ)\n",
        "    else:\n",
        "      values, indices = torch.topk(prediction,len(prediction),dim=0)\n",
        "      print(\"top_indices:{}\".format(indices))\n",
        "      indexes = indices[:,0].tolist()\n",
        "      each_summ = []\n",
        "      for ind in indexes:\n",
        "        each_summ.append(test_df['sentences'][i][ind])\n",
        "      each_summ = ' '.join(map(str, each_summ))\n",
        "      topsentences_summary.append(each_summ)\n",
        "      print(each_summ)\n",
        "\n",
        "    # Move logits to CPU\n",
        "    logits = outputs.logits.detach().cpu().numpy()\n",
        "    print(\"Logits:{}\".format(logits))\n",
        "\n",
        "print(\"summary:{}\".format(topsentences_summary))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRwNTTlBBvXW"
      },
      "source": [
        "#Function to create answer lists required\n",
        "def create_answer_df(test_df,summaries):\n",
        "  qid_test_df = test_df['id']\n",
        "  type_test_df = test_df['type']\n",
        "  summaries_test_df = summaries\n",
        "  return qid_test_df, type_test_df, summaries_test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU5LfaAfCIF-"
      },
      "source": [
        "qid_test_df, type_test_df, summaries_test_df = create_answer_df(test_df, topsentences_summary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcQ70mPYC1qh"
      },
      "source": [
        "#Join the answers in required format of BioASQ\n",
        "question_details = []\n",
        "for i in range(len(qid_test_df)):\n",
        "  dicti = {\"id\" : qid_test_df[i], \"ideal_answer\" : summaries_test_df[i], \"exact_answer\" : \"yes\"}\n",
        "  question_details.append(dicti)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0mF069HDAFb"
      },
      "source": [
        "#Converting teh data into JSON file as needed.\n",
        "import json\n",
        "\n",
        "x = {\"questions\" : question_details}\n",
        "\n",
        "# Serializing json \n",
        "json_object = json.dumps(x, indent = 2)\n",
        "\n",
        "with open('/content/drive/MyDrive/Thesis_B/BIOASQ_8b_batch1_bertbase_results_final.json', 'w') as outfile:\n",
        "    outfile.write(json_object)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}